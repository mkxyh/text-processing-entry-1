{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208f9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5fff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57fbdd5",
   "metadata": {},
   "source": [
    "#### Let's clean the text below by stemming, removing stop words and using `clean_str(raw_text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd09f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1=\"88*&6$#456999976674452345 Another normalization technique we'll apply is stemming, using the nltk 988756 h %6$@@56(*8^43@)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe48e1",
   "metadata": {},
   "source": [
    "#### Text stemming using `nltk.stem SnowballStemmer` :\n",
    "*  --Another normalization technique we'll apply is stemming, using the nltk port stemmer and lemmintizer.\n",
    "     The reason we are using this library is becaue we need to normalize our text input.\n",
    "*    Here is a good article that explains the technique: https://towardsdatascience.com/stemming-corpus-with-nltk-7a6a6d02d3e5\n",
    "*    We'll use snowball stemmer.\n",
    "\n",
    "#### example from the article posted above:\n",
    "`from nltk.stem import SnowballStemmer`\n",
    "\n",
    "`snowball = SnowballStemmer(language='english')`\n",
    "\n",
    "* As mentioned in the article above some words like `amazing` stem to nonsensical words like `amaz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa95dd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amaz'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from nltk.stem import SnowballStemmer\n",
    ">>> snowball = SnowballStemmer(language='english')\n",
    ">>> snowball.stem('amazing') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4646de",
   "metadata": {},
   "source": [
    "#### Removing stopwords article: https://pythonspot.com/nltk-stop-words/\n",
    "* Another technique to help normalize our text is by removing stopwords.\n",
    "* Stopwords are words that occour very often. \n",
    "\n",
    "*Quote from article above: \"\" The stopwords are a list of words that are very very common but don’t provide useful information for most text analysis procedures. While it is helpful for understand the structure of sentences, it does not help you understand the semantics of the sentences themselves. Here’s a list of most commonly used words in English:\n",
    "\"\" `N = [ 'stop', 'the', 'to', 'and', 'a', 'in', 'it', 'is', 'I', 'that', 'had', 'on', 'for', 'were', 'was']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d52e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/m9/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f692a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f5b740",
   "metadata": {},
   "source": [
    "#### Text cleaning :\n",
    "* Removes numbers and special chars using regex `re`\n",
    "* strips and turns text into lower case.\n",
    "* removes punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7939a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str2(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9+\\./()!?\\'\\`%$]\", \" \", string)\n",
    "    return string.replace(\"\\\\n\", \" \")\n",
    "\n",
    "def text_process(mess):\n",
    "    nopunc =[char for char in mess if char not in string.punctuation]\n",
    "    nopunc=''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    string cleaning (partially modified)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9()!?\\'\\`%$]\", \" \", string) \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\$\", \" $ \", string) #yes, isolate $\n",
    "    string = re.sub(r\"\\%\", \" % \", string) #yes, isolate %\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"(^|\\W)\\d+\", \"\", string)\n",
    "    # fixing XX X and xxx like as word\n",
    "    string = re.sub(r'\\S*(x{2,}|X{2,})\\S*',\"xxx\",string)\n",
    "    # removing non ascii\n",
    "    string = re.sub(r'[^\\x00-\\x7F]+', \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def pre_process_text(text):\n",
    "    text=clean_str(text)\n",
    "    text=snowball.stem(text)\n",
    "    text=' '.join(text_process(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281ad92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'another normalization technique apply stemming using nltk h'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process_text(example_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb10d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
